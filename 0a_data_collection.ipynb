{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, time\n",
    "import json\n",
    "import requests\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = datetime.datetime.utcfromtimestamp(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection of Reddit Scraping via Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection process will be done through Pushshift's API to scrape both askphilosophy and psychonauts subreddits. The loop starts from 2 days before todays date. The scrape will go until 15000 posts are collecetted with 2 second rests between each requests. The collected data will be added to a dataframe and saved. This occurs for each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Credit to Gwen Rathgeber/Ben Mathis! Thank you so much.\n",
    "\n",
    "subreddits = ['Psychonaut','askphilosophy']\n",
    "kind = \"submission\"  # we want text posts\n",
    "\n",
    "# Establish URL base\n",
    "BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\"\n",
    "\n",
    "last_date = datetime.datetime.utcfromtimestamp(time.time())     #utc from timestamp -50_000\n",
    "posts = {}  #empty dictionary\n",
    "for subreddit in subreddits:\n",
    "    posts[subreddit] = []\n",
    "    day = 2                  #start with the most recent post\n",
    "    cumulative_posts = 0\n",
    "    while cumulative_posts < 15000:                           #scrape 15,000 b/c minimum is 10,000 and some will be junk from what you scrape\n",
    "        stem = f\"{BASE_URL}?subreddit={subreddit}&size=100\"   #part of query, #will scrape from 100 posts\n",
    "        URL = f\"{stem}&after={day}d\"                           #will scrape from after the day we scrape it\n",
    "        print(\"Querying from: \" + URL)\n",
    "        try:                                                  #we use try, except b/c scraping from the web, you'll get a lot of errors\n",
    "            res = requests.get(URL)\n",
    "            assert res.status_code == 200\n",
    "            json = res.json()['data']\n",
    "            df = pd.DataFrame(json)\n",
    "            posts[subreddit].append(df)\n",
    "            cumulative_posts += df.shape[0]\n",
    "            final_date_pulled = datetime.datetime.utcfromtimestamp(df.iloc[-1, df.columns.get_loc('created_utc')])\n",
    "            increment = (last_date - final_date_pulled).days + 1\n",
    "            increment = increment if increment > 0 else 1\n",
    "            day += increment\n",
    "            last_date = final_date_pulled\n",
    "            print('successful')\n",
    "        except:\n",
    "            print(f'Scrape for {URL}, {day} failed')\n",
    "\n",
    "        time.sleep(2)                    #this is a delay in between scrapes\n",
    "\n",
    "print(\"Query complete!\")\n",
    "\n",
    "psychonaut_frame = pd.concat(posts['Psychonaut'])\n",
    "philosophy_frame = pd.concat(posts['askphilosophy'])\n",
    "\n",
    "psychonaut_frame.to_csv('./data/raw_psychonaut_initial_scrape.csv')\n",
    "philosophy_frame.to_csv('./data/raw_askphilosophy_initial_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
